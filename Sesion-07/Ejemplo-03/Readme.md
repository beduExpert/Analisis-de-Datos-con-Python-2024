üè† [**Inicio**](../../Readme.md) ‚û°Ô∏è / üìñ [**Sesi√≥n 07**](../Readme.md) ‚û°Ô∏è / üìù `Ejemplo 03: Procesamiento de lenguaje natural (NLP)`

## üéØ Objetivo

El objetivo es comprender c√≥mo las computadoras pueden interactuar y entender el lenguaje humano de manera eficiente. A medida que aumenta la cantidad de datos textuales generados por la comunicaci√≥n digital, como las rese√±as de productos o comentarios en redes sociales, se hace necesario convertir estos textos en datos estructurados que se puedan analizar.

---

## üöÄ Comencemos

El **procesamiento de lenguaje natural (NLP)** es una rama de la inteligencia artificial que permite a las computadoras entender y interactuar con el lenguaje humano, convirtiendo textos en datos estructurados que pueden ser analizados para extraer informaci√≥n valiosa. Con el creciente volumen de datos textuales en plataformas digitales, el NLP facilita el an√°lisis de opiniones, la clasificaci√≥n de correos electr√≥nicos, la traducci√≥n de idiomas, la extracci√≥n de informaci√≥n y la creaci√≥n de chatbots, mejorando la eficiencia en procesos que dependen del lenguaje y permitiendo automatizar tareas repetitivas.

---

### üõ†Ô∏è **Text y FreqDist de NLTKr**

**NLTK (Natural Language Toolkit)** es una biblioteca de Python para procesamiento de lenguaje natural, que ofrece herramientas para analizar y manipular texto. 
- **Text** permite trabajar con texto tokenizado, facilitando tareas como b√∫squeda de palabras en contexto, an√°lisis de frecuencia, y visualizaci√≥n de patrones en el lenguaje. 
- **FreqDist** se utiliza para calcular la distribuci√≥n de frecuencias de los elementos en una lista, como palabras en un texto. Facilita tareas como el an√°lisis de frecuencia de palabras, identificaci√≥n de las palabras m√°s comunes, y exploraci√≥n de la diversidad l√©xica en el lenguaje.

Para aplicar esto, utilizaremos el archivo [Ejemplo_01_03_Amazon_Sales_Dataset.csv](../../Datasets/S07/Ejemplo_01_03_Amazon_Sales_Dataset.csv) que utilizamos en el ejemplo 01. Hagamos los siguientes pasos:

1. **Instalar NLTK:** Si no lo tienes instalado, puedes hacerlo ejecutando.
    ```python
    # Instalar NLTK
    !pip install nltk

    # Descargar los recursos necesarios
    nltk.download('punkt')
    ```

2. **Cargar los datos y preprocesar:**

    ```python
    # Aseg√∫rate de tener nltk instalado y descarga los recursos necesarios
    import nltk
    nltk.download('punkt')

    import pandas as pd
    import string
    from nltk.text import Text
    from nltk import FreqDist
    from nltk.tokenize import word_tokenize

    # Cargar el archivo CSV
    df = pd.read_csv('/Datos_ejemplo01y03_WorkS07_amazon_sales_dataset.csv')

    # Preprocesamiento: Convertir a min√∫sculas y eliminar signos de puntuaci√≥n
    df['Review_clean'] = df['Review'].str.lower().str.translate(str.maketrans('', '', string.punctuation))
    ```

    **df:**
    - Funci√≥n **.str.lower():** convierte todo el texto de la columna "Review" a min√∫sculas, lo que ayuda a unificar el texto y evitar que palabras como "Producto" y "producto" se traten como diferentes.
    - Funci√≥n **.str.translate(str.maketrans('', '', string.punctuation)):** elimina todos los signos de puntuaci√≥n del texto, como comas, puntos, signos de interrogaci√≥n, etc. Esto se hace para limpiar el texto y facilitar su an√°lisis, eliminando caracteres que no son necesarios para la mayor√≠a de las tareas de procesamiento de lenguaje natural.

    <br>

3. **Tokenizaci√≥n:**

    ```python
    # Tokenizar las rese√±as
    tokens = word_tokenize(' '.join(df['Review_clean'].dropna()))
    ```

    - **word_tokenize:** es una funci√≥n de la biblioteca NLTK que divide un texto en palabras individuales o "tokens". Esta t√©cnica se llama Tokenizaci√≥n.
    - **df['Review_clean'].dropna():** Este comando toma la columna Review_clean del DataFrame df y elimina cualquier valor nulo que pueda existir.
    - **' '.join(df['Review_clean'].dropna()):** Une todas las rese√±as limpias en una sola cadena de texto grande, separando cada rese√±a con un espacio.
    - **word_tokenize(...):** Aplica la tokenizaci√≥n al texto unido, dividiendo esta gran cadena en una lista de palabras individuales (tokens).

    <br>

4. **Crear un objeto Text y calcular la distribuci√≥n de frecuencias:**

    ```python
    # Crear un objeto Text de NLTK
    text_nltk = Text(tokens)

    # Funcionalidades de Text
    # 1. Buscar palabras en contexto
    text_nltk.concordance('wonderful')

    # 2. Buscar palabras similares
    text_nltk.similar('product')

    # 3. Generar dispersi√≥n de palabras
    # Nota: Esta l√≠nea produce un gr√°fico que se mostrar√° en tu entorno local
    text_nltk.dispersion_plot(['product', 'quality', 'recommend', 'value'])

    # 4. Contar el total de palabras y la cantidad de palabras √∫nicas
    total_tokens = len(text_nltk)
    unique_tokens = len(set(text_nltk))
    print(f"Total de tokens: {total_tokens}")
    print(f"Cantidad de tokens √∫nicos: {unique_tokens}")
    ```

    **Con Text, tenemos varias funcionalidades, algunas de ellas son:**
    - **Buscar palabras en contexto**
    text_nltk.concordance('palabra'): Muestra las ocurrencias de una palabra en el texto junto con su contexto, lo que es √∫til para entender c√≥mo se utiliza esa palabra.

    - **Buscar palabras similares**
    text_nltk.similar('palabra'): Encuentra palabras que aparecen en contextos similares a la palabra especificada, lo que puede ayudar a descubrir sin√≥nimos o t√©rminos relacionados en el texto.

    - **Generar dispersi√≥n de palabras**
    text_nltk.dispersion_plot(['palabra1', 'palabra2']): Crea un gr√°fico de dispersi√≥n que muestra las posiciones de las palabras en el texto, √∫til para analizar la distribuci√≥n de t√©rminos clave a lo largo de un documento.

    - **Contar el total de palabras y la cantidad de palabras √∫nicas**
    len(text_nltk): Devuelve el n√∫mero total de tokens en el texto, √∫til para medir el tama√±o del corpus.
    len(set(text_nltk)): Calcula el n√∫mero de palabras √∫nicas en el texto, lo que da una idea de la diversidad l√©xica.

    <br>

5. **Crear un objeto FreqDist y funcionalidades:**

    ```python
    #FreqDisq y funcionalidades

    # Crear un objeto FreqDist para an√°lisis de frecuencia de palabras
    fdist = FreqDist(text_nltk)

    # Funcionalidades de FreqDist
    # 1. Mostrar todas las palabras √∫nicas
    unique_words = list(fdist.keys())
    print(f"Palabras √∫nicas (muestra): {unique_words[:10]}")  # Muestra solo las primeras 10 palabras √∫nicas

    # 2. Filtrar palabras seg√∫n su frecuencia (hapaxes)
    hapaxes = fdist.hapaxes()
    print(f"Hapaxes (muestra): {hapaxes[:10]}")  # Muestra solo las primeras 10 hapaxes

    # 3. Gr√°fico de palabras m√°s comunes
    # Nota: Esta l√≠nea produce un gr√°fico que se mostrar√° en tu entorno local
    fdist.plot(30, cumulative=False)

    # 4. Acceder a la frecuencia de una palabra espec√≠fica
    frequency_of_product = fdist['product']
    print(f"Frecuencia de la palabra 'product': {frequency_of_product}")
    ```

    **Con FreqDist, tenemos varias funcionalidades, algunas de ellas son:**
    - **Mostrar todas las palabras √∫nicas**
    fdist.keys(): Devuelve todas las palabras √∫nicas encontradas en el texto.

    - **Filtrar palabras seg√∫n su frecuencia**
    fdist.hapaxes(): Devuelve una lista de palabras que solo aparecen una vez en el texto, conocidas como "hapax legomena".
    fdist.plot(30, cumulative=False): Genera un gr√°fico de las 30 palabras m√°s comunes y su frecuencia, lo que ayuda a visualizar la distribuci√≥n de palabras.

    - **Acceder a la frecuencia de una palabra espec√≠fica**
    fdist['palabra']: Proporciona la frecuencia de una palabra espec√≠fica en el texto, lo que es √∫til para an√°lisis m√°s detallados.


---

### üí° **¬øSab√≠as que?...**

El **procesamiento de lenguaje natural (NLP)** es una disciplina que combina la ling√º√≠stica, la inform√°tica y la inteligencia artificial para permitir a las computadoras entender, interpretar y generar lenguaje humano de manera eficiente. Algunas aplicaciones comunes de NLP incluyen:

- **An√°lisis de sentimientos:** Identificar emociones y opiniones en textos, como rese√±as de productos o comentarios en redes sociales.
- **Extracci√≥n de informaci√≥n:** Identificar y extraer informaci√≥n relevante de textos, como nombres de personas, fechas o lugares.
- **Traducci√≥n autom√°tica:** Convertir texto de un idioma a otro, facilitando la comunicaci√≥n entre personas que hablan diferentes idiomas.
- **Generaci√≥n de texto:** Crear contenido de manera autom√°tica, como res√∫menes de texto, respuestas a preguntas o di√°logos de chatbots.

Pr√°cticamente es la base de los LLMs (Large Language Models) como GPT-3, BERT, entre otros.

---

‚¨ÖÔ∏è [**Anterior**](../Readme.md) | [**Siguiente**](../Reto-03/Readme.md) ‚û°Ô∏è