üè† [**Inicio**](../../Readme.md) ‚û°Ô∏è / üìñ [**Sesi√≥n 05**](../Readme.md) ‚û°Ô∏è / üìù `Ejemplo 03: T√©cnicas de evaluaci√≥n de modelos`


## üéØ Objetivo 
Comprender y aplicar t√©cnicas de evaluaci√≥n de modelos predictivos usando m√©tricas como precisi√≥n, recall y F1-score. Tambi√©n aprender√°s a dividir datos en conjuntos de entrenamiento y prueba para evaluar el modelo de manera justa y mejorar la toma de decisiones basada en datos.

---

## üöÄ Comencemos

Antes de evaluar modelos, es crucial entender la divisi√≥n de datos:

- **Un dataset de entrenamiento** es un subconjunto de datos que se utiliza para entrenar un modelo predictivo, es decir, el modelo aprende de estos datos para identificar patrones y relaciones entre las variables.  Se utiliza para ajustar el modelo y aprender de los datos.

- **Un dataset de prueba** es otro subconjunto que se utiliza para evaluar el rendimiento del modelo una vez que ha sido entrenado. Este conjunto de prueba contiene datos que el modelo no ha visto antes, permitiendo medir su capacidad de generalizaci√≥n y asegurando que las predicciones no est√°n sesgadas hacia los datos de entrenamiento. Se utiliza para evaluar el modelo y comprobar su rendimiento en datos no vistos.


Dividir los datos en entrenamiento y prueba asegura que el modelo generalice correctamente, evitando el sobreajuste y proporcionando una evaluaci√≥n m√°s realista de su rendimiento en situaciones reales. Evaluar con un conjunto de prueba independiente ayuda a identificar problemas y medir c√≥mo se comportar√° el modelo en la pr√°ctica.

---

### üõ†Ô∏è **Creaci√≥n de un dataset de entrenamiento y prueba**
Supongamos que eres un qu√≠mico interesado en modelado y simulaci√≥n, y tienes un dataset con diferentes compuestos qu√≠micos y su eficacia en una reacci√≥n particular. Vamos a dividir este dataset en conjuntos de entrenamiento y prueba para entrenar un modelo predictivo y luego evaluar su rendimiento. Utilizaremos el dataset [Ejemplo_03_04_Dataset_Quimico.csv](../../Datasets/S05/Ejemplo_03_04_Dataset_Quimico.csv) 

1. **Dividir el dataset en entrenamiento y prueba**

    ```python
    from sklearn.model_selection import train_test_split
    import pandas as pd

    # Cargar el dataset
    df = pd.read_csv('../Ejemplo_03_04_Dataset_Quimico.csv') # Cambiar la ruta del archivo si es necesario

    # Definir caracter√≠sticas (X) y la etiqueta objetivo (y)
    X = df.drop('Eficacia_Reaccion', axis=1)  # Caracter√≠sticas
    y = df['Eficacia_Reaccion']  # Etiqueta objetivo

    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

    # Mostrar la forma de los conjuntos para confirmar la divisi√≥n
    print(f"Conjunto de Entrenamiento: {X_train.shape}, {y_train.shape}")
    print(f"Conjunto de Prueba: {X_test.shape}, {y_test.shape}")
    ```

    - **Cargar el dataset**: El dataset simulado se carga desde un archivo CSV.
    - Definir caracter√≠sticas y etiqueta: Se separan las columnas del dataset en X (las caracter√≠sticas) y y (la etiqueta o el objetivo que queremos predecir).
    - **Dividir el dataset**: Usamos train_test_split de scikit-learn para dividir el dataset en un conjunto de entrenamiento y otro de prueba, manteniendo el 70% de los datos para entrenar el modelo y el 30% para evaluarlo. La opci√≥n stratify=y asegura que se mantenga la proporci√≥n original de la etiqueta en ambos conjuntos.
    - **Confirmar la divisi√≥n**: Finalmente, se imprimen las formas de los conjuntos de entrenamiento y prueba para confirmar que la divisi√≥n se realiz√≥ correctamente.

---

üåü **T√©cnicas de evaluaci√≥n de modelos predictivos:** 
Las t√©cnicas y m√©tricas de evaluaci√≥n son herramientas clave para medir el rendimiento de un modelo predictivo. Entre las m√©tricas m√°s comunes est√°n:

- üîç **Precisi√≥n:** Proporci√≥n de predicciones positivas correctas.
- üéØ **Recall:** Capacidad del modelo para identificar todas las instancias positivas.
- ‚öñÔ∏è **F1-Score:** Media arm√≥nica entre precisi√≥n y recall, balanceando ambas m√©tricas.

La elecci√≥n de t√©cnicas de evaluaci√≥n depende del tipo de problema (clasificaci√≥n, regresi√≥n, clustering, etc.). En el ejemplo que revisaremos, usaremos un **√Årbol de Decisi√≥n**, un modelo que se estructura como un √°rbol donde cada nodo representa una pregunta sobre una caracter√≠stica del dataset. No te preocupes por entender el √°rbol de decisi√≥n ahora, nos centraremos en la evaluaci√≥n.

---

### üõ†Ô∏è **Aplicaci√≥n de t√©cnicas de evaluaci√≥n de modelos predictivos**
Vamos a implementar estas m√©tricas utilizando el dataset `Ejemplo_03_04_Dataset_Quimico` que usamos anteriormente. Entrenaremos un modelo y luego mediremos su rendimiento usando precisi√≥n, recall y F1-score.

1. **Cargar y preparar el dataset:** Como siempre, aseg√∫rate de tener tus bibliotecas importadas y tu dataset cargado correctamente. Ahora tambi√©n dividiremos los datos en conjunto de entrenamiento `(70%)` y prueba `(30%)` como lo aprendimos anteriormente.

    ```python
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.metrics import precision_score, recall_score, f1_score
    ```
    Utilizaremos:
    - pandas para manejar y procesar datos estructurados.
    - train_test_split de Scikit-learn para dividir datos en conjuntos de entrenamiento y prueba.
    - DecisionTreeClassifier de Scikit-learn para crear y entrenar modelos de √°rboles de decisi√≥n.
    - precision_score, recall_score, y f1_score de Scikit-learn para evaluar el rendimiento de modelos de clasificaci√≥n.

        
    ```python
    # Cargar el dataset
    df = pd.read_csv('../Ejemplo_03_04_Dataset_Quimico .csv') # Cambiar la ruta del archivo si es necesario

    # Definir caracter√≠sticas (X) y la etiqueta objetivo (y)
    X = df.drop('Eficacia_Reaccion', axis=1) # Caracter√≠sticas
    y = df['Eficacia_Reaccion'] # Etiqueta objetivo

    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    ```
2.  **Entrenar un modelo:** Usaremos un √°rbol de decisi√≥n como modelo, aunque puedes experimentar con otros modelos tambi√©n.

    ```python
    # Crear y entrenar el modelo
    model = DecisionTreeClassifier(random_state=42)
    model.fit(X_train, y_train)
    ```
    - **Crear y entrenar un modelo:** Creamos un modelo de √°rbol de decisi√≥n y lo entrenamos con los datos de entrenamiento.

3.  **Realizar predicciones y evaluar el modelo:** Despu√©s de entrenar el modelo, realizaremos predicciones en el conjunto de prueba y luego calcularemos las m√©tricas de evaluaci√≥n.
    ```python
    # Realizar predicciones en el conjunto de prueba
    y_pred = model.predict(X_test)

    # Calcular las m√©tricas
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Mostrar los resultados
    print(f'Precisi√≥n: {precision:.2f}')
    print(f'Recall: {recall:.2f}')
    print(f'F1-Score: {f1:.2f}')
    ```
    Dependiendo de los resultados, podr√≠as decidir ajustar el modelo, cambiar de algoritmo o recopilar m√°s datos para mejorar el rendimiento. Este proceso te ayuda a crear un modelo que no solo funcione bien en datos conocidos, sino que tambi√©n sea capaz de hacer predicciones precisas en nuevos escenarios.

---

### üìâ **Interpretaci√≥n de la distribuci√≥n muestral**
Para hacer la interpretaci√≥n a nuestro resultados, los niveles de aceptaci√≥n o criterios comunes para las m√©tricas de Precisi√≥n, Recall y F1-score son:


| Precisi√≥n (Precision) | Recall (Sensibilidad) | F1-Score |
|----------------|-----------------------|-----------------------|
| **> 0.90 (90%):** Alta precisi√≥n. El modelo rara vez comete falsos positivos. | **> 0.90 (90%):** Alto recall. El modelo casi siempre captura los verdaderos positivos. | **> 0.90 (90%):** Excelente balance entre precisi√≥n y recall. El modelo es altamente confiable en general. |
| **0.80 - 0.90 (80% - 90%):** Buena precisi√≥n. El modelo es confiable, pero todav√≠a hay algunos falsos positivos. | **0.80 - 0.90 (80% - 90%):** Buen recall. El modelo identifica correctamente la mayor√≠a de los casos positivos. | **0.80 - 0.90 (80% - 90%):** Buen balance, el modelo maneja bien tanto la precisi√≥n como el recall. |
| **0.60 - 0.79 (60% - 79%):** Precisi√≥n aceptable, pero puede haber un n√∫mero significativo de falsos positivos. | **0.60 - 0.79 (60% - 79%):** Recall aceptable, pero podr√≠a estar perdiendo un n√∫mero considerable de verdaderos positivos. | **0.60 - 0.79 (60% - 79%):** F1-score aceptable, pero el modelo puede necesitar ajustes para mejorar en precisi√≥n o recall. |
| **< 0.60 (60%):** Baja precisi√≥n. El modelo a menudo predice incorrectamente como positivo algo que no lo es. | **< 0.60 (60%):** Bajo recall. El modelo no logra identificar muchos de los casos positivos. | **< 0.60 (60%):** Bajo F1-score. El modelo probablemente necesita mejoras significativas. |


üîÑ **Los valores pueden variar seg√∫n el contexto, pero los criterios mencionados son un buen punto de partida.**

Al ejecutar el c√≥digo, obtuvimos:

- üéØ **Precisi√≥n: 0.50**  
El modelo tiene un 50% de precisi√≥n en predicciones positivas, lo que indica una alta cantidad de falsos positivos. En la mitad de los casos, predice √©xitos que en realidad no lo son.

- üîç **Recall: 0.58**  
El modelo identifica correctamente solo el 58% de los √©xitos reales, perdiendo el 42% de las reacciones exitosas (falsos negativos).

- ‚öñÔ∏è **F1-Score: 0.54**  
Este valor refleja un desequilibrio entre precisi√≥n y recall, se√±alando un rendimiento general bajo.

üìâ **En resumen, el modelo tiene problemas para hacer predicciones precisas y completas, con errores significativos tanto en falsos positivos como en falsos negativos.**

üí° **¬øQu√© podemos hacer?**

1. üõ†Ô∏è **Mejorar la calidad de los datos:** Revisar outliers, variables irrelevantes o caracter√≠sticas que necesiten transformaci√≥n.
2. ‚ûï **Recopilar m√°s datos:** Si el dataset es peque√±o, m√°s datos podr√≠an ayudar al modelo a aprender mejor.

---

‚¨ÖÔ∏è [**Anterior**](../Readme.md) | [**Siguiente**](../Reto-02/Readme.md) ‚û°Ô∏è