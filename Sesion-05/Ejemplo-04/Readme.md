üè† [**Inicio**](../../Readme.md) ‚û°Ô∏è / üìñ [**Sesi√≥n 05**](../Readme.md) ‚û°Ô∏è / üìù `Ejemplo 04: M√©todos de validaci√≥n cruzada`


## üéØ Objetivo 
El objetivo es comprender y aplicar t√©cnicas de validaci√≥n cruzada para evaluar la capacidad de generalizaci√≥n y rendimiento de los modelos predictivos, utilizando m√©todos como K-Fold Cross-Validation, Leave-One-Out Cross-Validation (LOOCV), y Stratified K-Fold Cross-Validation. Tambi√©n se busca aprender a implementar estos m√©todos de manera efectiva, asegurando que la evaluaci√≥n del modelo sea robusta y representativa.

---

## üöÄ Comencemos
**La validaci√≥n cruzada** es una t√©cnica para evaluar el rendimiento y capacidad de generalizaci√≥n de un modelo predictivo, dividiendo los datos en subconjuntos para entrenar y probar el modelo en diferentes combinaciones. Esto ayuda a prevenir el overfitting y asegura un buen desempe√±o en datos no vistos.

**M√©todos de validaci√≥n cruzada:**

- **K-Fold Cross-Validation:** Divide el dataset en K partes. El modelo se entrena K veces, usando K-1 partes para entrenamiento y 1 para prueba. Los resultados se promedian para una estimaci√≥n precisa.

- **Leave-One-Out Cross-Validation (LOOCV):** Un caso especial de K-Fold donde K es igual al n√∫mero de muestras. Cada muestra se usa una vez como prueba, ofreciendo una evaluaci√≥n muy precisa, pero con alto costo computacional.

- **Stratified K-Fold Cross-Validation:** Variante de K-Fold que mantiene la proporci√≥n original de clases en cada fold, ideal para problemas de clasificaci√≥n con clases desbalanceadas.

---

### üõ†Ô∏è **Ejecuci√≥n de t√©cnicas de validaci√≥n cruzada**
Antes de aplicar t√©cnicas como `K-Fold`, `LOOCV`, y `Stratified K-Fold`, es crucial entender la necesidad de evaluar un modelo de m√∫ltiples formas. Dividir los datos en entrenamiento y prueba una sola vez puede no ser representativo debido a la variabilidad de los datos. La validaci√≥n cruzada ofrece una evaluaci√≥n m√°s robusta al promediar resultados de diferentes divisiones, proporcionando una estimaci√≥n m√°s precisa del rendimiento del modelo.

Vamos a ejecutar `K-Fold`, `LOOCV`, y `Stratified K-Fold` utilizando el ejemplo del qu√≠mico interesado en modelado y simulaci√≥n [Ejemplo_03_04_Dataset_Quimico.csv](../../Datasets/S05/Ejemplo_03_04_Dataset_Quimico.csv)

1. **Pasos iniciales:** ¬øLos recuerdas? Instalar (en caso de que no las tengamos) e importar las librer√≠as, cargar y preparar el dataset. 

    ```python
    !pip install pandas scikit-learn
    ```
    ```python
    import pandas as pd
    from sklearn.model_selection import train_test_split

    # Cargar el dataset
    df = pd.read_csv('../Ejemplo_03_04_Dataset_Quimico.csv') # Cambiar la ruta del archivo si es necesario

    # Definir caracter√≠sticas (X) y la etiqueta objetivo (y)
    X = df.drop('Eficacia_Reaccion', axis=1)  # Caracter√≠sticas
    y = df['Eficacia_Reaccion']  # Etiqueta objetivo

    # Convertir las variables categ√≥ricas en variables dummy si es necesario
    X = pd.get_dummies(X, drop_first=True)
    ```

2. **K-Fold Cross-Validation:** Implementamos K-Fold Cross-Validation usando Scikit-learn.

    ```python
    from sklearn.model_selection import cross_val_score, KFold
    from sklearn.tree import DecisionTreeClassifier

    # Crear el modelo
    model = DecisionTreeClassifier(random_state=42)

    # Implementar K-Fold Cross-Validation
    kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-Folds
    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

    # Mostrar los resultados
    print(f"K-Fold Cross-Validation Scores: {scores}")
    print(f"Promedio de las puntuaciones: {scores.mean():.2f}")
    ```
    - `KFold(n_splits=5)`: Divide los datos en 5 partes.
    - `cross_val_score`: Calcula la precisi√≥n del modelo en cada fold.
    - `scores.mean()`: Promedia los resultados para obtener una medida m√°s robusta.

  <br>

3.  **Leave-One-Out Cross-Validation (LOOCV):** Implementamos LOOCV, que es m√°s intensivo computacionalmente.
    ```python
    from sklearn.model_selection import LeaveOneOut

    # Crear el modelo
    model = DecisionTreeClassifier(random_state=42)

    # Implementar LOOCV
    loo = LeaveOneOut()
    scores = cross_val_score(model, X, y, cv=loo, scoring='accuracy')

    # Mostrar los resultados
    print(f"LOOCV Mean Score: {scores.mean():.2f}")
    ```
    - `LeaveOneOut()`: Deja una muestra fuera en cada iteraci√≥n.
    - `cross_val_score`: Calcula la precisi√≥n para cada iteraci√≥n y luego promedia los resultados.

  <br>

4.  **Stratified K-Fold Cross-Validation:** Implementamos Stratified K-Fold Cross-Validation para asegurar que cada fold tenga una distribuci√≥n equilibrada de clases.

    ```python
    from sklearn.model_selection import StratifiedKFold

    # Crear el modelo
    model = DecisionTreeClassifier(random_state=42)

    # Implementar Stratified K-Fold Cross-Validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')

    # Mostrar los resultados
    print(f"Stratified K-Fold Cross-Validation Scores: {scores}")
    print(f"Promedio de las puntuaciones: {scores.mean():.2f}")
    ```
    - `StratifiedKFold(n_splits=5)`: Divide los datos en 5 partes, manteniendo la proporci√≥n de clases en cada fold.
    - `cross_val_score`: Calcula la precisi√≥n en cada fold y luego promedia los resultados.

    Al utilizar K-Fold, LOOCV, y Stratified K-Fold, obtienes una visi√≥n completa del rendimiento del modelo en diferentes escenarios, lo que te ayuda a elegir el mejor modelo para tus datos.

---

### üìâ **Interpretaci√≥n de la distribuci√≥n muestral**
Para evaluar los m√©todos de validaci√≥n cruzada revisados es importante considerar varios criterios que te ayudar√°n a determinar cu√°l es el m√°s adecuado para tu problema y c√≥mo interpretar sus resultados.

<div align="center">
<h4>K-Fold Cross-Validation, criterios:</h4>
</div>

| Variabilidad de los Scores | Promedio de los Scores | N√∫mero de Folds (K) |
|----------------------------|------------------------|---------------------|
| Eval√∫a c√≥mo var√≠an los puntajes de precisi√≥n, recall, F1-score, etc., entre los diferentes folds. | El promedio de los puntajes te da una idea del rendimiento general del modelo. | Aumentar el n√∫mero de folds generalmente da una estimaci√≥n m√°s precisa, pero a costa de m√°s tiempo de c√≥mputo. |
| **Baja variabilidad:** Indica que el modelo es estable y que su rendimiento no depende mucho de c√≥mo se dividen los datos. | **Alto promedio:** Sugiere que el modelo es eficaz en general. | **K m√°s alto (10 o m√°s):** Proporciona una mejor estimaci√≥n, pero aumenta el tiempo de procesamiento. |
| **Alta variabilidad:** Sugiere que el modelo puede estar sobreajustado (overfitting) o que los datos son ruidosos. | **Bajo promedio:** Indica que el modelo tiene dificultades para predecir correctamente. | **K bajo (5):** Reduce el tiempo de c√≥mputo, pero puede dar una estimaci√≥n menos estable. |

---

<div align="center">
<h4>Leave-One-Out Cross-Validation (LOOCV), criterios:</h4>
</div>

| Exactitud de la estimaci√≥n                                                                 | Costo computacional                                                                 | Variabilidad en los Scores                                                                                     |
|--------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| LOOCV es √∫til cuando necesitas una estimaci√≥n muy precisa del rendimiento del modelo porque utiliza casi todos los datos disponibles para entrenar en cada iteraci√≥n. | LOOCV puede ser computacionalmente costoso, especialmente en datasets grandes, porque entrena y prueba el modelo tantas veces como muestras haya. | Dado que solo se deja una muestra fuera en cada iteraci√≥n, el rendimiento puede fluctuar m√°s si los datos son ruidosos o contienen outliers. |
| **Alta exactitud:** Buen m√©todo si el dataset es peque√±o y cada muestra es valiosa.        | **Apropiado para datasets peque√±os:** Se recomienda cuando el dataset no es muy grande |                                                                                                                |
| **Bajo rendimiento:** Puede ocurrir si el modelo es muy sensible a peque√±as variaciones en los datos. | **Costoso para datasets grandes:** Puede ser ineficiente y lento                                                  |                                                                                                                |

---

<div align="center">
<h4>Stratified K-Fold Cross-Validation, criterios:</h4>
</div>

| Representatividad de las clases                                                                 | Consistencia de los Scores                                                                 | Variabilidad en los Scores                                                                                                            |
|-------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|
| Este m√©todo es ideal para problemas de clasificaci√≥n con clases desbalanceadas, ya que asegura que cada fold tenga una proporci√≥n similar de cada clase. | Evaluar c√≥mo de consistentes son los puntajes entre los folds. En un problema de clasificaci√≥n desbalanceado, este m√©todo deber√≠a proporcionar puntajes m√°s consistentes. | Al igual que en K-Fold, el promedio de los scores proporciona una estimaci√≥n del rendimiento general del modelo, pero con la ventaja de que las clases est√°n equilibradas en cada fold. |
| **Buena representatividad:** Si las clases est√°n desbalanceadas, este m√©todo es m√°s confiable para evaluar el rendimiento del modelo. | **Alta consistencia:** Indica que el modelo maneja bien las clases desbalanceadas.                                                 |                                                                                                                                      |
| **Mala representatividad:** Si no se usa este m√©todo en problemas desbalanceados, los resultados pueden ser enga√±osos.              | **Baja consistencia:** Sugiere que el modelo podr√≠a estar sesgado hacia la clase mayoritaria.                                       |                                                                                                                                      |

---

**En nuestro ejemplo, los resultados fueron:**

**‚Ä¢ K-Fold Cross-Validation**  
- üìù **Scores:** [0.5, 0.25, 0.6, 0.7, 0.45]  
  Las precisiones var√≠an significativamente entre los 5 folds, con un m√≠nimo de 0.25 y un m√°ximo de 0.70.
- üìä **Promedio de las puntuaciones:** 0.50  
  El modelo acierta en un 50% de las veces, sugiriendo un rendimiento mediocre y posible sensibilidad a la divisi√≥n de los datos, lo que podr√≠a indicar overfitting o underfitting.

**‚Ä¢ LOOCV**  
- üìâ **Mean Score:** 0.45  
  El modelo acert√≥ en el 45% de los casos, ligeramente inferior al promedio de K-Fold, lo que sugiere dificultades para generalizar.

**‚Ä¢ Stratified K-Fold Cross-Validation**  
- üìä **Scores:** [0.45, 0.4, 0.55, 0.55, 0.55]  
  Los puntajes son m√°s consistentes, oscilando entre 0.40 y 0.55.
- üìà **Promedio de las puntuaciones:** 0.50  
  Al igual que con K-Fold, el promedio es 0.50, mostrando rendimiento promedio cuando las clases est√°n equilibradas en cada fold.

**üîç En resumen‚Ä¶**  
Tenemos un rendimiento global **Moderado/Bajo**, con una precisi√≥n promedio alrededor del 50% en todos los m√©todos de validaci√≥n cruzada, lo que indica dificultades para hacer predicciones precisas y confiables.

---
### üí° **¬øSab√≠as que?...**

El **overfitting** ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento, capturando incluso el ruido y las irregularidades, lo que lo hace menos efectivo para predecir nuevos datos. Esto es como aprenderse un libro de memoria en lugar de entender su contenido. 

Por otro lado, el **underfitting** sucede cuando un modelo es demasiado simple para captar los patrones importantes de los datos, fallando tanto en los datos de entrenamiento como en los nuevos. Es como estudiar solo los res√∫menes de un libro y no entenderlo realmente.

---

‚¨ÖÔ∏è [**Anterior**](../Readme.md) | [**Siguiente**](../../Sesion-06/Readme.md) ‚û°Ô∏è